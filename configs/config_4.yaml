# DPO Training Configuration
# configs/config_4.yaml

# Training hyperparameters
lr: 3.0e-5
epochs: 10
batch_size: 4
beta: 0.1
weight_decay: 0.01

# Model and data configuration
max_absolute_length: 2048
validation_split: 0.1
model_name: "microsoft/Phi-3.5-mini-instruct"
data_path: "data/prepared_training_data_temp1.5_bsz8_shot2_q5000.pt"

# Device configuration
device: "cuda"
cuda_device: "0"

# Wandb configuration
wandb_project: "dpo-training"
wandb_entity: "kenotron"

# Random seed
random_seed: 42